{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM807DL6vALlebAmctEirYT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammedAdheeb/Tamil-BiGRU/blob/main/BiGRU_Tamil_ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TG2yTybbcr9U",
        "outputId": "634becdb-d2bc-47e6-c36a-3d6f6263d052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.12 in /usr/local/lib/python3.11/dist-packages (2.12.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12) (0.37.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.12.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12) (0.4.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow==2.12 librosa jiwer soundfile\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Bidirectional, GRU, Dense, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#previous approach to download and extract, discontinued because the dataset was downloaded in the drive\n",
        "# # Define dataset URL and paths\n",
        "# dataset_url = \"https://openslr.org/resources/127/mile_tamil_asr_corpus.tar.gz\"\n",
        "# dataset_path = \"/content/mile_tamil_asr_corpus.tar.gz\"\n",
        "# extracted_path = \"/content/mile_tamil_asr_corpus\"\n",
        "\n",
        "# # Download the dataset\n",
        "# urllib.request.urlretrieve(dataset_url, dataset_path)\n",
        "\n",
        "# # Extract the dataset\n",
        "# with tarfile.open(dataset_path, \"r:gz\") as tar:\n",
        "#     tar.extractall(path=extracted_path)\n",
        "\n",
        "# print(\"Dataset downloaded and extracted successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqj-4JaseTqz",
        "outputId": "008f4300-389f-4f46-edb4-628815a14ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded and extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "tar_file_path = \"/content/drive/My Drive/mile_tamil_asr_corpus.tar.gz\"  # Update with actual file path\n",
        "extracted_path = \"/content\"\n",
        "\n",
        "# Ensure the extraction directory exists\n",
        "os.makedirs(extracted_path, exist_ok=True)\n",
        "\n",
        "# Extract the dataset\n",
        "with tarfile.open(tar_file_path, \"r:gz\") as tar:\n",
        "    tar.extractall(path=extracted_path)\n",
        "\n",
        "print(\"Dataset extracted successfully at:\", extracted_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DKCvwbZe-o1",
        "outputId": "3fd47f28-0750-436b-84d7-ebca9d2911b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Dataset extracted successfully at: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to audio and transcript directories\n",
        "train_audio_dir = os.path.join(extracted_path, \"mile_tamil_asr_corpus/train/audio_files\")\n",
        "train_trans_dir = os.path.join(extracted_path, \"mile_tamil_asr_corpus/train/trans_files\")\n",
        "\n",
        "test_audio_dir = os.path.join(extracted_path, \"mile_tamil_asr_corpus/test/audio_files\")\n",
        "test_trans_dir = os.path.join(extracted_path, \"mile_tamil_asr_corpus/test/trans_files\")\n",
        "\n",
        "# Count .wav and .txt files\n",
        "train_wav_count = len([f for f in os.listdir(train_audio_dir) if f.endswith(\".wav\")])\n",
        "train_txt_count = len([f for f in os.listdir(train_trans_dir) if f.endswith(\".txt\")])\n",
        "\n",
        "test_wav_count = len([f for f in os.listdir(test_audio_dir) if f.endswith(\".wav\")])\n",
        "test_txt_count = len([f for f in os.listdir(test_trans_dir) if f.endswith(\".txt\")])\n",
        "\n",
        "print(f\"Training set: {train_wav_count} .wav files, {train_txt_count} .txt files\")\n",
        "print(f\"Testing set: {test_wav_count} .wav files, {test_txt_count} .txt files\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QJusZmwpSr9",
        "outputId": "c1c24356-2bf5-48da-a182-152d537b97ce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: 77314 .wav files, 77314 .txt files\n",
            "Testing set: 12087 .wav files, 12087 .txt files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_transcripts(trans_dir):\n",
        "    transcripts = {}\n",
        "    for trans_file in os.listdir(trans_dir):\n",
        "        if trans_file.endswith(\".txt\"):\n",
        "            audio_id = os.path.splitext(trans_file)[0]  # Extract audio ID from filename\n",
        "            file_path = os.path.join(trans_dir, trans_file)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                transcript = f.read().strip()  # Read the entire file as the transcript\n",
        "                transcripts[audio_id] = transcript\n",
        "    return transcripts\n",
        "\n",
        "# Load train and test transcripts\n",
        "train_transcripts = load_transcripts(train_trans_dir)\n",
        "test_transcripts = load_transcripts(test_trans_dir)\n",
        "\n",
        "print(f\"Loaded {len(train_transcripts)} training transcripts.\")\n",
        "print(f\"Loaded {len(test_transcripts)} testing transcripts.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPcx9tfAikzc",
        "outputId": "2c933e8f-19cf-48e0-8008-b9d543731165"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 77314 training transcripts.\n",
            "Loaded 12087 testing transcripts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "def extract_mfcc(audio_path, n_mfcc=40):\n",
        "    \"\"\"\n",
        "    Extract MFCC features from an audio file.\n",
        "\n",
        "    Args:\n",
        "        audio_path (str): Path to the audio file.\n",
        "        n_mfcc (int): Number of MFCC coefficients to extract.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: MFCC features with shape (time_steps, num_features).\n",
        "    \"\"\"\n",
        "    # Load the audio file\n",
        "    y, sr = librosa.load(audio_path, sr=None)  # sr=None preserves the original sampling rate\n",
        "\n",
        "    # Extract MFCC features\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "\n",
        "    # Transpose to shape (time_steps, num_features)\n",
        "    return mfccs.T\n",
        "\n",
        "def calculate_max_audio_len(audio_dir):\n",
        "    max_len = 0\n",
        "    for audio_file in os.listdir(audio_dir):\n",
        "        if audio_file.lower().endswith(\".wav\"):\n",
        "            audio_path = os.path.join(audio_dir, audio_file)\n",
        "            mfccs = extract_mfcc(audio_path)\n",
        "            max_len = max(max_len, len(mfccs))\n",
        "    return max_len\n",
        "\n",
        "# Function to calculate max_text_len\n",
        "def calculate_max_text_len(transcripts, char_to_idx):\n",
        "    max_len = 0\n",
        "    for label in transcripts.values():\n",
        "        encoded_label = [char_to_idx[char] for char in label]\n",
        "        max_len = max(max_len, len(encoded_label))\n",
        "    return max_len"
      ],
      "metadata": {
        "id": "moX7YQjW61DU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# previous appproach to load data, not cotinued because of the excessive memory usage\n",
        "# def load_audio_data(audio_dir, transcripts):\n",
        "#     audio_data = []\n",
        "#     labels = []\n",
        "#     for audio_file in os.listdir(audio_dir):\n",
        "#         if audio_file.lower().endswith(\".wav\"):\n",
        "#             audio_id = os.path.splitext(audio_file)[0]  # Extract audio ID from filename\n",
        "#             if audio_id in transcripts:  # Ensure transcript exists\n",
        "#                 audio_path = os.path.join(audio_dir, audio_file)\n",
        "#                 mfccs = extract_mfcc(audio_path)  # Extract MFCC features\n",
        "#                 audio_data.append(mfccs)\n",
        "#                 labels.append(transcripts[audio_id])\n",
        "#     return audio_data, labels\n",
        "\n",
        "# # Load train and test audio data\n",
        "# train_audio_data, train_labels = load_audio_data(train_audio_dir, train_transcripts)\n",
        "# test_audio_data, test_labels = load_audio_data(test_audio_dir, test_transcripts)\n",
        "\n",
        "# print(f\"Loaded {len(train_audio_data)} training audio files.\")\n",
        "# print(f\"Loaded {len(test_audio_data)} testing audio files.\")"
      ],
      "metadata": {
        "id": "nxvUPnpup8Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def audio_generator(audio_dir, transcripts, char_to_idx, batch_size, max_audio_len, max_text_len):\n",
        "    \"\"\"\n",
        "    Generator to yield batches of audio data and encoded labels.\n",
        "    \"\"\"\n",
        "    audio_files = [f for f in os.listdir(audio_dir) if f.lower().endswith(\".wav\")]\n",
        "    num_files = len(audio_files)\n",
        "\n",
        "    for i in range(0, num_files, batch_size):\n",
        "        batch_audio_data = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for j in range(i, min(i + batch_size, num_files)):\n",
        "            audio_file = audio_files[j]\n",
        "            audio_id = os.path.splitext(audio_file)[0]\n",
        "\n",
        "            if audio_id in transcripts:\n",
        "                # Load audio data\n",
        "                audio_path = os.path.join(audio_dir, audio_file)\n",
        "                mfccs = extract_mfcc(audio_path)\n",
        "                batch_audio_data.append(mfccs)\n",
        "\n",
        "                # Encode transcript\n",
        "                label = transcripts[audio_id]\n",
        "                encoded_label = [char_to_idx[char] for char in label]\n",
        "                batch_labels.append(encoded_label)\n",
        "\n",
        "        # Pad sequences dynamically\n",
        "        batch_padded_audio = pad_sequences(batch_audio_data, maxlen=max_audio_len, dtype=\"float32\", padding=\"post\")\n",
        "        batch_padded_labels = pad_sequences(batch_labels, maxlen=max_text_len, padding=\"post\")\n",
        "\n",
        "        yield batch_padded_audio, batch_padded_labels"
      ],
      "metadata": {
        "id": "7cXAuJFMMaDd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build character vocabulary\n",
        "all_text = \" \".join(list(train_transcripts.values()) + list(test_transcripts.values()))\n",
        "unique_chars = sorted(set(all_text))\n",
        "char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "char_to_idx[\"<PAD>\"] = 0  # Padding token\n",
        "idx_to_char[0] = \"<PAD>\"\n",
        "\n",
        "print(f\"Vocabulary size: {len(char_to_idx)}\")\n",
        "\n",
        "# Calculate max lengths\n",
        "train_audio_dir = os.path.join(extracted_path, \"mile_tamil_asr_corpus/train/audio_files\")\n",
        "test_audio_dir = os.path.join(extracted_path, \"mile_tamil_asr_corpus/test/audio_files\")\n",
        "\n",
        "max_audio_len = max(calculate_max_audio_len(train_audio_dir), calculate_max_audio_len(test_audio_dir))\n",
        "max_text_len = max(calculate_max_text_len(train_transcripts, char_to_idx), calculate_max_text_len(test_transcripts, char_to_idx))\n",
        "\n",
        "print(f\"Max audio length: {max_audio_len}\")\n",
        "print(f\"Max text length: {max_text_len}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2e8hKl0J4rv",
        "outputId": "9c3aaccd-cd69-4ed7-94fe-ba08c06d0ff1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 49\n",
            "Max audio length: 1215\n",
            "Max text length: 605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model parameterss\n",
        "input_dim = 40  # Number of MFCC features\n",
        "num_classes = len(char_to_idx)  # Vocabulary size\n",
        "\n",
        "# Input layer\n",
        "inputs = Input(shape=(None, input_dim))\n",
        "\n",
        "# Bi-directional GRU layers\n",
        "x = Bidirectional(GRU(256, return_sequences=True))(inputs)\n",
        "x = Bidirectional(GRU(256, return_sequences=True))(x)\n",
        "\n",
        "# Dense layer for character prediction\n",
        "outputs = TimeDistributed(Dense(num_classes, activation=\"softmax\"))(x)\n",
        "\n",
        "# Define model\n",
        "model = Model(inputs, outputs)\n",
        "\n",
        "# Compile with CTC loss\n",
        "def ctc_loss(y_true, y_pred):\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "\n",
        "    loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n",
        "    return loss\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=ctc_loss)\n",
        "model.summary()\n",
        "\n",
        "# Create generators\n",
        "batch_size = 32\n",
        "train_gen = audio_generator(train_audio_dir, train_transcripts, char_to_idx, batch_size, max_audio_len, max_text_len)\n",
        "test_gen = audio_generator(test_audio_dir, test_transcripts, char_to_idx, batch_size, max_audio_len, max_text_len)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=test_gen,\n",
        "    steps_per_epoch=len(os.listdir(train_audio_dir)) // batch_size,\n",
        "    validation_steps=len(os.listdir(test_audio_dir)) // batch_size,\n",
        "    epochs=20,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5Ui7qD-cPnL1",
        "outputId": "f029ec72-707e-46c6-b936-ae31742ae116"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None, 40)]        0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, None, 512)        457728    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, None, 512)        1182720   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDis  (None, None, 49)         25137     \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,665,585\n",
            "Trainable params: 1,665,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node 'ctc_loss/CTCLoss' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-10-018f1a0ad348>\", line 39, in <cell line: 0>\n      history = model.fit(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"<ipython-input-8-018f1a0ad348>\", line 27, in ctc_loss\n      loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/backend.py\", line 7050, in ctc_batch_cost\n      tf.compat.v1.nn.ctc_loss(\nNode: 'ctc_loss/CTCLoss'\nSaw a non-null label (index >= num_classes - 1) following a null label, batch: 0 num_classes: 49 labels: 23,39,1,9,24,48,24,44,1,33,38,20,48,20,40,33,38,20,40,1,3,33,28,48,15,31,44,25,48,25,37,28,48,15,48,15,38,30,40,26,48,1,7,24,48,24,44,1,25,30,22,48,22,22,40,26,48,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 labels seen so far: 23,39,1,9,24\n\t [[{{node ctc_loss/CTCLoss}}]] [Op:__inference_train_function_23561]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-018f1a0ad348>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     keras_symbolic_tensors = [\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'ctc_loss/CTCLoss' defined at (most recent call last):\n    File \"<frozen runpy>\", line 198, in _run_module_as_main\n    File \"<frozen runpy>\", line 88, in _run_code\n    File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n      ColabKernelApp.launch_instance()\n    File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-10-018f1a0ad348>\", line 39, in <cell line: 0>\n      history = model.fit(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1051, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n      return self.compiled_loss(\n    File \"/usr/local/lib/python3.11/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/losses.py\", line 142, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/losses.py\", line 268, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"<ipython-input-8-018f1a0ad348>\", line 27, in ctc_loss\n      loss = tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)\n    File \"/usr/local/lib/python3.11/dist-packages/keras/backend.py\", line 7050, in ctc_batch_cost\n      tf.compat.v1.nn.ctc_loss(\nNode: 'ctc_loss/CTCLoss'\nSaw a non-null label (index >= num_classes - 1) following a null label, batch: 0 num_classes: 49 labels: 23,39,1,9,24,48,24,44,1,33,38,20,48,20,40,33,38,20,40,1,3,33,28,48,15,31,44,25,48,25,37,28,48,15,48,15,38,30,40,26,48,1,7,24,48,24,44,1,25,30,22,48,22,22,40,26,48,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 labels seen so far: 23,39,1,9,24\n\t [[{{node ctc_loss/CTCLoss}}]] [Op:__inference_train_function_23561]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Added a newer implementation for audio as well , so discontinued\n",
        "# Build character vocabulary\n",
        "# all_text = \" \".join(list(train_transcripts.values()) + list(test_transcripts.values()))\n",
        "# unique_chars = sorted(set(all_text))\n",
        "# char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
        "# idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "# char_to_idx[\"<PAD>\"] = 0  # Padding token\n",
        "# idx_to_char[0] = \"<PAD>\"\n",
        "\n",
        "# print(f\"Vocabulary size: {len(char_to_idx)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0vVMfLwOjNJ",
        "outputId": "cf090e0b-d72e-46c5-ca5d-cb4f532825e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defined befor the audio_generator part, discontinued\n",
        " # Build character vocabulary\n",
        "# all_text = \" \".join(list(train_transcripts.values()) + list(test_transcripts.values()))\n",
        "# unique_chars = sorted(set(all_text))\n",
        "# char_to_idx = {char: idx + 1 for idx, char in enumerate(unique_chars)}\n",
        "# idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "# char_to_idx[\"<PAD>\"] = 0  # Padding token\n",
        "# idx_to_char[0] = \"<PAD>\"\n",
        "\n",
        "# # Encode transcripts into sequences of indices\n",
        "# def encode_transcripts(labels):\n",
        "#     encoded = []\n",
        "#     for label in labels:\n",
        "#         encoded.append([char_to_idx[char] for char in label])\n",
        "#     return encoded\n",
        "\n",
        "# train_encoded_labels = encode_transcripts(train_labels)\n",
        "# test_encoded_labels = encode_transcripts(test_labels)\n",
        "\n",
        "# print(f\"Vocabulary size: {len(char_to_idx)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "skV-ki_KzJV6",
        "outputId": "9e3e3403-d0f3-419b-89d9-1ddf244b5b5c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_labels' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-79ab387076dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_encoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_transcripts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mtest_encoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_transcripts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create generators\n",
        "train_gen = audio_generator(train_audio_dir, train_transcripts, char_to_idx, batch_size)\n",
        "test_gen = audio_generator(test_audio_dir, test_transcripts, char_to_idx, batch_size)\n",
        "\n",
        "# Test the generator\n",
        "for batch_audio, batch_labels in train_gen:\n",
        "    print(f\"Batch audio shape: {batch_audio.shape}\")\n",
        "    print(f\"Batch labels shape: {batch_labels.shape}\")\n",
        "    break  # Process only one batch for testing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "w-4Uh_rAO6RB",
        "outputId": "892ca96b-0d3d-42d2-a880-1e650554d810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'max_audio_len' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-8e754ab8f9c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Test the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_audio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch audio shape: {batch_audio.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch labels shape: {batch_labels.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-22b8b64e6368>\u001b[0m in \u001b[0;36maudio_generator\u001b[0;34m(audio_dir, transcripts, char_to_idx, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Pad sequences dynamically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mbatch_padded_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_audio_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_audio_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mbatch_padded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_text_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'max_audio_len' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# # Pad sequences\n",
        "# max_audio_len = max(len(seq) for seq in train_audio_data + test_audio_data)\n",
        "# max_text_len = max(len(seq) for seq in train_encoded_labels + test_encoded_labels)\n",
        "\n",
        "# train_padded_audio = pad_sequences(train_audio_data, maxlen=max_audio_len, dtype=\"float32\", padding=\"post\")\n",
        "# test_padded_audio = pad_sequences(test_audio_data, maxlen=max_audio_len, dtype=\"float32\", padding=\"post\")\n",
        "\n",
        "# train_padded_labels = pad_sequences(train_encoded_labels, maxlen=max_text_len, padding=\"post\")\n",
        "# test_padded_labels = pad_sequences(test_encoded_labels, maxlen=max_text_len, padding=\"post\")\n",
        "\n",
        "# print(f\"Padded training audio shape: {train_padded_audio.shape}\")\n",
        "# print(f\"Padded training labels shape: {train_padded_labels.shape}\")"
      ],
      "metadata": {
        "id": "c6_tU7i01SIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Function to dynamically pad sequences in batches\n",
        "def dynamic_padding(audio_data, labels, batch_size, max_audio_len, max_text_len):\n",
        "    padded_audio_batches = []\n",
        "    padded_label_batches = []\n",
        "\n",
        "    for i in range(0, len(audio_data), batch_size):\n",
        "        batch_audio = audio_data[i:i + batch_size]\n",
        "        batch_labels = labels[i:i + batch_size]\n",
        "\n",
        "        # Pad each batch to the maximum length within the batch\n",
        "        batch_padded_audio = pad_sequences(batch_audio, maxlen=max_audio_len, dtype=\"float32\", padding=\"post\")\n",
        "        batch_padded_labels = pad_sequences(batch_labels, maxlen=max_text_len, padding=\"post\")\n",
        "\n",
        "        padded_audio_batches.append(batch_padded_audio)\n",
        "        padded_label_batches.append(batch_padded_labels)\n",
        "\n",
        "    return padded_audio_batches, padded_label_batches\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Dynamically pad sequences\n",
        "train_padded_audio_batches, train_padded_labels_batches = dynamic_padding(\n",
        "    train_audio_data, train_encoded_labels, batch_size, max_audio_len, max_text_len\n",
        ")\n",
        "test_padded_audio_batches, test_padded_labels_batches = dynamic_padding(\n",
        "    test_audio_data, test_encoded_labels, batch_size, max_audio_len, max_text_len\n",
        ")\n",
        "\n",
        "print(f\"Number of training batches: {len(train_padded_audio_batches)}\")\n",
        "print(f\"Number of testing batches: {len(test_padded_audio_batches)}\")"
      ],
      "metadata": {
        "id": "Q_N_SwqODfVe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}